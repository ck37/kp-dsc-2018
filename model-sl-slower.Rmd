---
title: "Model: SL basic"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

# Load an initial pair of startup functions.
source("R/_startup.R")
# Load necessary libraries; set auto_install = TRUE to try to install any needed packages.
startup(auto_install = FALSE, verbose = FALSE)
# Load all .R files in the R/ subdirectory.
ck37r::load_all_code("R", verbose = TRUE)

# File created in clean.Rmd
load("data/clean.RData")

# Define a model name specifically for this Rmd file.
task$model_name = "sl-slower"
```

## Define estimators

```{r define_estimators}

# This will select only the top 15 covariates based on correlation with the outcome.
screen.corRank15 = function(...) screen.corRank2(..., rank = 15)
screen.corRank25 = function(...) screen.corRank2(..., rank = 25)

# Create 9 combinations of settings for xgboost.
xgb_grid =
  create.Learner("SL.xgboost_fast", name_prefix = "xgb",
                 detailed_names = TRUE,
                 tune = list(
                   # TODO: conduct more extensive hyperparameter tuning.
                   shrinkage = c(0.001, 0.01, 0.1),
                   max_depth = c(1, 4, 8)
                 ))
xgb_grid$names

# Setup parallel backend for glmnet_fast.
doParallel::registerDoParallel(cores = RhpcBLASctl::get_num_cores())

# Add screeners so that it's not as slow to estimate.
# TODO: test wider numbers of covariates, e.g. 15 or all.
# TODO: grid search on xgboost hyperparameters.
sl_lib =
  c(lapply(xgb_grid$names, function(name) c(name, "screen.corRank15", "screen.corRank25")),
    list(c("SL.xgboost_fast", "screen.corRank15", "screen.corRank25"),
         c("SL.ranger_fast", "screen.corRank15", "screen.corRank25"),
         c("SL.ranger_fast", "screen.corRank15", "screen.corRank25"),
         c("SL.glmnet_fast", "screen.corRank15", "screen.corRank25"),
         "SL.mean"))

```

## Run estimation

```{r superlearner}

# Set a multicore-compatible seed for reproducibility.
set.seed(3137033, "L'Ecuyer-CMRG")
sl =
  SuperLearner(Y = task$outcome,
               X = task$data[, task$covariates],
               family = binomial(),
               verbose = TRUE,
               SL.library = sl_lib,
               # TODO: consider method = nnlogLik
               cvControl = SuperLearner.CV.control(V = 5L))
               # Re-run with 20 folds when we want to finalize our model.
               # cvControl = SuperLearner.CV.control(V = 20L))

sl

cat("Execution time:", round(sl$times$everything["elapsed"] / 60, 1), "minutes.\n")

# Save our results and our task for posterity.
save(sl, task,
     file = paste0("data/model-", task$model_name, ".RData"))
```

## Review model

```{r review_model}
# Review auc of the learners.
ck37r::auc_table(sl, y = task$outcome)
# TODO: export table.

# Plot ROC curve.
ck37r::plot_roc(sl, y = task$outcome)
ggsave(paste0("visuals/roc-", task$model_name, ".png"))

```

## Predict on test

TODO: convert more of this to a general function so that we can use across Rmd files.

```{r test_prediction}
test = data.table::fread("data-raw/test.csv", data.table = FALSE)
dim(test)

(names(test) = tolower(names(test)))

# Restrict to columns that we want.
test_df = test[, task$covariates]

# Replace -999 values with NA for all of our covariates.
# TODO: convert to a function that we re-use code between model.Rmd and clean.Rmd.
test_df[, task$covariates] <- lapply(test_df[, task$covariates], function(col) {
  col[col == -999] <- NA
  col
})

# Impute missing values.
# TODO: should use imputation medians from the training data rather the test data.
impute_test = ck37r::impute_missing_values(test_df[, task$covariates], verbose = TRUE)
test_df[, task$covariates] = impute_test$data

# Apply SuperLearner to generate predictions.
system.time({
  predictions =
    predict(sl, test_df,
            # Only estimate the models that are used in the SL ensemble.
            onlySL = TRUE,
            # Allow multithreaded prediction to speed this up.
            num.threads = RhpcBLASctl::get_num_cores())$pred
})

# Review prediction distribution.
summary(predictions)
qplot(predictions) + theme_minimal()
ggsave(paste0("visuals/test-hist-", task$model_name, ".png"))

# Create a dataframe that contains just what we need to submit an entry.
# $eventid is the primary id for each observation.
export = data.frame(EventId = test$eventid, Label = predictions)

# Convert probability prediction to a class prediction.
# TODO: run optimal threshold analysis to decide best probability threshold.
export$Label = ifelse(export$Label > 0.5, "s", "b")

# Review predicted class labels.
table(export$Label)
prop.table(table(export$Label))

# Generate a csv file to upload to competition submission page.
rio::export(export,
            file = paste0("exports/submission-", task$model_name, ".csv"))

```
